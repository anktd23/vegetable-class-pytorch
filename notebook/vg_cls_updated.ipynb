{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Vegetable Image Classification Using Pytorch"
      ],
      "metadata": {
        "id": "Juh8DuIiVzH6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import the required libraries and functions."
      ],
      "metadata": {
        "id": "wexb1O7iV0jF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "iLD9krpIPRI6"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "import torchvision.transforms as tt\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torch.utils.data.dataloader import DataLoader\n",
        "import os\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install opendatasets --upgrade --quiet"
      ],
      "metadata": {
        "id": "OmNn1YgIPzwN"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Downloading and preparing the data."
      ],
      "metadata": {
        "id": "e6l5NsVwV75W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import opendatasets as od\n",
        "dataset_url = 'https://www.kaggle.com/misrakahmed/vegetable-image-dataset'\n",
        "od.download(dataset_url)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p46nXgGBPSIM",
        "outputId": "c190d94e-d36d-46c7-9fe8-467d4bad001d"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Skipping, found downloaded files in \"./vegetable-image-dataset\" (use force=True to force download)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "train_path = \"/content/vegetable-image-dataset/Vegetable Images/train\"\n",
        "val_path = \"/content/vegetable-image-dataset/Vegetable Images/validation\"\n",
        "test_path = \"/content/vegetable-image-dataset/Vegetable Images/test\"\n"
      ],
      "metadata": {
        "id": "OEGDE24JRk62"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training & Evaluating the Model"
      ],
      "metadata": {
        "id": "rJpAZToGWCQM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Model Architecture"
      ],
      "metadata": {
        "id": "qW9uzGlTXsQp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Network(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Network, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 20, 5, 1) \n",
        "        self.conv2 = nn.Conv2d(20, 50, 5, 1)\n",
        "        self.fc1 = nn.Linear(50 * 53 * 53, 500)\n",
        "        self.fc2 = nn.Linear(500, 15)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.max_pool2d(x, 2, 2)\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = F.max_pool2d(x, 2, 2)\n",
        "        x = x.view(-1, 50 * 53 * 53)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return F.log_softmax(x, dim=1)"
      ],
      "metadata": {
        "id": "Y9i4MOsqWMfD"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Functions to train,validate and evaluate the model"
      ],
      "metadata": {
        "id": "vcXOiCHXX2GW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, device, train_loader, optimizer, epoch):\n",
        "  model.train()\n",
        "  for batch_idx, (data, target) in enumerate(train_loader):\n",
        "    data, target = data.to(device), target.to(device)\n",
        "    optimizer.zero_grad()\n",
        "    output = model(data)\n",
        "    loss = F.nll_loss(output, target)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if batch_idx % 100 == 0:\n",
        "       print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
        "                100. * batch_idx / len(train_loader), loss.item()))"
      ],
      "metadata": {
        "id": "V4ojoOZTWPZR"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def validate(model, device, val_loader):\n",
        "    model.eval()\n",
        "    val_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in val_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            val_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
        "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    val_loss /= len(val_loader.dataset)\n",
        "\n",
        "    print('\\nValidation set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        val_loss, correct, len(val_loader.dataset),\n",
        "        100. * correct / len(val_loader.dataset)))"
      ],
      "metadata": {
        "id": "MfoSoD8vWU47"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test(model, device, test_loader):\n",
        "    model.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            output = model(data)\n",
        "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
        "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "\n",
        "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "        test_loss, correct, len(test_loader.dataset),\n",
        "        100. * correct / len(test_loader.dataset)))"
      ],
      "metadata": {
        "id": "IXrMr9deWYbF"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def main():\n",
        "\n",
        "  #using gpu if available\n",
        "  device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "  batch_size = 64\n",
        "\n",
        "  stats = ((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "\n",
        "  #image augmentation ans transformation\n",
        "  trainTfms = tt.Compose([ tt.RandomHorizontalFlip(), \n",
        "                          tt.ToTensor(), \n",
        "                          tt.Normalize(*stats,inplace=True),\n",
        "                          tt.Resize((224,224))\n",
        "                        ] )\n",
        "  validTestTfms = tt.Compose([tt.ToTensor(), \n",
        "                              tt.Normalize(*stats),\n",
        "                              tt.Resize((224,224))])\n",
        "\n",
        "\n",
        "  train_ds = ImageFolder(train_path, trainTfms)\n",
        "  test_ds = ImageFolder(test_path, validTestTfms)\n",
        "  valid_ds = ImageFolder(val_path, validTestTfms)\n",
        "\n",
        "  batch_size = 32\n",
        "\n",
        "  #Dataloader\n",
        "  train_dl = DataLoader(train_ds, batch_size, shuffle=True, num_workers=3, pin_memory=True)\n",
        "  valid_dl = DataLoader(valid_ds, batch_size, num_workers=3, pin_memory=True)\n",
        "  test_dl = DataLoader(test_ds, batch_size*2, num_workers=3, pin_memory=True)\n",
        "\n",
        "  #Model Training,Validation & Evaluation\n",
        "  model = Network().to(device)\n",
        "  optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5)\n",
        "\n",
        "  for epoch in range(2):\n",
        "    train(model, device, train_dl, optimizer, epoch)\n",
        "    validate(model, device,valid_dl)\n",
        "    test(model, device, test_dl)\n",
        "\n",
        "  # Specify the output directory\n",
        "  output_dir = \"/content/output\"\n",
        "  os.makedirs(output_dir, exist_ok=True)\n",
        "\n",
        "  #saving model\n",
        "  model_path = os.path.join(output_dir, \"torch_model.pt\")\n",
        "  torch.save(model.state_dict(), model_path)\n",
        "  print(\"Model saved successfully at:\", model_path)\n"
      ],
      "metadata": {
        "id": "N8HKyc4URdtx"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7iygeSPrR8eX",
        "outputId": "b05e30d5-a5a0-43f5-9a9d-665378ceaf3d"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train Epoch: 0 [0/15000 (0%)]\tLoss: 2.705142\n",
            "Train Epoch: 0 [3200/15000 (21%)]\tLoss: 1.160661\n",
            "Train Epoch: 0 [6400/15000 (43%)]\tLoss: 1.022711\n",
            "Train Epoch: 0 [9600/15000 (64%)]\tLoss: 0.940720\n",
            "Train Epoch: 0 [12800/15000 (85%)]\tLoss: 1.118714\n",
            "\n",
            "Validation set: Average loss: 0.8461, Accuracy: 2205/3000 (74%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.8428, Accuracy: 2184/3000 (73%)\n",
            "\n",
            "Train Epoch: 1 [0/15000 (0%)]\tLoss: 0.687459\n",
            "Train Epoch: 1 [3200/15000 (21%)]\tLoss: 0.569877\n",
            "Train Epoch: 1 [6400/15000 (43%)]\tLoss: 0.453306\n",
            "Train Epoch: 1 [9600/15000 (64%)]\tLoss: 0.161433\n",
            "Train Epoch: 1 [12800/15000 (85%)]\tLoss: 0.364464\n",
            "\n",
            "Validation set: Average loss: 0.3907, Accuracy: 2644/3000 (88%)\n",
            "\n",
            "\n",
            "Test set: Average loss: 0.3824, Accuracy: 2634/3000 (88%)\n",
            "\n",
            "Model saved successfully at: /content/output/torch_model.pt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "meTehDgZR8pd"
      },
      "execution_count": 10,
      "outputs": []
    }
  ]
}